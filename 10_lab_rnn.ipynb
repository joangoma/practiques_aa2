{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joangoma/practiques_aa2/blob/main/10_lab_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgvzqC1IBjH"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "## Lab credit\n",
        "Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2019).\n",
        "\n",
        "Updated by [Gerard I. GÃ¡llego](https://www.linkedin.com/in/gerard-gallego/) (2021), [Javier Ferrando](https://www.linkedin.com/in/javierferrandomonsonis/) (2022), and [Ioannis Tsiamas](https://www.linkedin.com/in/i-tsiamas/) (2023).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqBBz46awSAh"
      },
      "source": [
        "# The Fault in Our Time\n",
        "\n",
        "This lab session introduces our beloved friends, the [recurrent neural networks (RNNs)](https://en.wikipedia.org/wiki/Recurrent_neural_network). Concretely, the topology we will be seeing is the Elman type, nowadays widely known plainly as RNN. Recurrent neural networks are the super cool queens of sequences: they know about order in sequences. As a quick test for how important sequential context is, and to prove that it is also very important for you... **CAN YOU TELL THE SIXTH DIGIT OF YOUR MOBILE PHONE NUMBER? WHAT PROCESS ARE YOU FOLLOWING TO RECALL IT?** Exactly, you went straight from the beginning of the full sequence, hence this is how important sequences are to us too :)\n",
        "\n",
        "As in the example before, we always work with sequences when using RNNs. In each batch of data, we have as many elements as the length of the sequence (seq_len), and each of these elements can contain multiple features (num_feats):\n",
        "\n",
        "<p align=\"center\"><br>\n",
        "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/input_batch.png?raw=true\" class=\"center\" title=\"input batch\" width=\"300\"/>\n",
        "</p><br>\n",
        "\n",
        "A fully connected layer is defined as:\n",
        "\n",
        "$\\boldsymbol{h}_t = \\tanh(\\boldsymbol{W}\\boldsymbol{x}_t + \\boldsymbol{b})$\n",
        "\n",
        "With \"only one\" (but super important) change we formulate the RNN:\n",
        "\n",
        "$\\boldsymbol{h}_t = \\tanh(\\boldsymbol{W}\\boldsymbol{x}_t + \\boldsymbol{U}\\boldsymbol{h}_{t-1} + \\boldsymbol{b})$\n",
        "\n",
        "Exactly, we added the matrix $\\boldsymbol{U}$ which is a set of connections among all the neurons from the hidden layers to themselves (hence a feedback)!\n",
        "\n",
        "This looks like the following, which is typically unrolled in time to show both flows of data, feed-forward ($\\boldsymbol{W}$) + time ($\\boldsymbol{U}$):\n",
        "\n",
        "<p align=\"center\"><br>\n",
        "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/one_layer_rnn.png?raw=true\" class=\"center\" title=\"one layer RNN\" width=\"300\"/>\n",
        "</p><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Os0axjRsVUrB"
      },
      "outputs": [],
      "source": [
        "# Let's first import the typical stuff to play with deep nets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "torch.manual_seed(1)\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  torch.cuda.manual_seed_all(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8YHdo2o0Ago"
      },
      "source": [
        "### Exercise 1: Building a recurrent neural layer\n",
        "\n",
        "In the next cell, we will define our own unidirectional RNN layer. The class `MyUnidirectionalRNN` must make use of `nn.Linear` layers to make the feed-forward and time projections, and use the `nn.Parameter` class to build the biases. Please build the recurrent neural component with the addition of the recurrent connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MXwfrhtaVUrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cedbd2-ff88-410a-9848-e37efe9ee614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Output shape: 5 sequences, each of length 15, each token with 32 dims\n"
          ]
        }
      ],
      "source": [
        "class MyUnidirectionalRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, num_feats, rnn_size=128):\n",
        "    super().__init__()\n",
        "    self.rnn_size = rnn_size\n",
        "\n",
        "    # Definition of the RNN parameters with the use of Linear layers:\n",
        "\n",
        "    # Define the input activation matrix W\n",
        "    self.W = nn.Linear(num_feats, rnn_size, bias=False)\n",
        "\n",
        "    # TODO: Define the hidden activation matrix U\n",
        "    self.U = nn.Linear(rnn_size, rnn_size, bias=False)\n",
        "\n",
        "    # Define the bias\n",
        "    self.b = nn.Parameter(torch.zeros(1, rnn_size))\n",
        "\n",
        "  def forward(self, x, state=None):\n",
        "    # Assuming x is of shape [batch_size, seq_len, num_feats]\n",
        "    xs = torch.chunk(x, x.shape[1], dim=1)\n",
        "    hts = []\n",
        "    if state is None:\n",
        "      state = self.init_state(x.shape[0])\n",
        "    ht = state\n",
        "    for xt in xs:\n",
        "      # turn x[t] into shape [batch_size, num_feats] to be projected\n",
        "      xt = xt.squeeze(1)\n",
        "      # RNN formulation\n",
        "      ht = F.tanh(self.W(xt) + self.U(ht) + self.b)\n",
        "      # give the temporal dimension back to h[t] to be cated\n",
        "      hts.append(ht.unsqueeze(1))\n",
        "    hts = torch.cat(hts, dim=1)\n",
        "    return hts\n",
        "\n",
        "  def init_state(self, batch_size):\n",
        "    return torch.zeros(batch_size, self.rnn_size)\n",
        "\n",
        "# To correctly assess the answer, we build an example RNN with 10 inputs and 32 neurons\n",
        "rnn = MyUnidirectionalRNN(10, 32)\n",
        "# Then we will forward some random sequences, each of length 15\n",
        "xt = torch.randn(5, 15, 10)\n",
        "# The returned tensor will be h[t]\n",
        "ht = rnn(xt)\n",
        "assert ht.shape[0] == 5 and ht.shape[1] == 15 and ht.shape[2] == 32, \\\n",
        "'Something went wrong within the RNN :('\n",
        "print('Success! Output shape: {} sequences, each of length {}, each '\\\n",
        "      'token with {} dims'.format(ht.shape[0], ht.shape[1], ht.shape[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnEZ6UmPAJU"
      },
      "source": [
        "### But Why Would You Do That?\n",
        "\n",
        "Congratz on finishing your first RNN definition! Now you should understand a bit more on the intrinsics of our sequential friends. But why would you define your own RNN? We didn't even operate with a GPU. We didn't even consider that possibility. So in the real world, we use PyTorch's `nn.RNN`, which allows for building a **stack of RNN layers directly**. Let's see some examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* NUM_FEATS = 10: This line sets the number of input features to 10. Each element or time step in the input sequence will have 10 features. Think of these features as different characteristics or pieces of information describing each element in the sequence.\n",
        "* SEQ_LEN = 25: This line sets the length of the input sequences to 25. This means each input to the RNN will be a sequence of 25 elements.\n",
        "* BATCH_SIZE = 5: This line sets the batch size to 5. The batch size determines how many sequences are processed by the RNN at the same time during training.\n",
        "* HIDDEN_SIZE = 128: This line sets the hidden size of the RNN to 128. The hidden size represents the number of neurons in the hidden layer of the RNN, which is responsible for capturing and remembering information from the input sequence."
      ],
      "metadata": {
        "id": "cYSTMlh9ahBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Gt1Vis5rVUrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27eefe7-fdcc-461c-81ea-86eb61c3ab99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(10, 128)\n",
            "Output h[t] tensor shape:  torch.Size([25, 5, 128])\n",
            "Output state tensor shape:  torch.Size([1, 5, 128])\n"
          ]
        }
      ],
      "source": [
        "# we will work with 10 input features\n",
        "NUM_FEATS = 10\n",
        "# and sequences of length 25\n",
        "SEQ_LEN = 25\n",
        "# and 5 samples per batch\n",
        "BATCH_SIZE = 5\n",
        "# and 128 neurons\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "# The first RNN contains a single layer\n",
        "rnn1 = nn.RNN(NUM_FEATS, HIDDEN_SIZE)\n",
        "print(rnn1)\n",
        "\n",
        "# Now let's build a random input tensor to forward through it\n",
        "xt = torch.randn(SEQ_LEN, BATCH_SIZE, NUM_FEATS)\n",
        "ht, state = rnn1(xt)\n",
        "print('Output h[t] tensor shape: ', ht.shape)\n",
        "print('Output state tensor shape: ', state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4c-TaLQ5Ag"
      },
      "source": [
        "#### OK STOP IT HERE, We've got to talk\n",
        "\n",
        "Think about how many things are happening in the previous cell. First, we define some hyper-parameters to define the input tensor shape and the RNN size. Then, we build one RNN layer. Then, we build random data. Finally, we forward the random data, and what is returned? Why does the input tensor `x` have that shape? Why is the RNN returning 2 output values?\n",
        "\n",
        "**First answer:** The input data to an RNN can be shaped in 2 formats: `batch_first=True` and `batch_first=False`. As its name indicates, when it is `False`, the `batch_size` dimension is not the first but the second one. Then which is the first one? The `sequence_length`. If we do not specify anything, by default `batch_first=False`, so the tensor $\\boldsymbol{x}_t$ must have the dimensions: [`seq_len`, `batch_size`, `num_feats`]. We normally use `batch_first=True` to couple the RNN easily with other layers like the `nn.Linear` one.\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "Find the second answer on \"**Why is the RNN returning 2 output values?**\". Understand what is the `state` output and answer: \"**what does it contain?**\". Your source of knowledge is in the following URL, where the outputs description for the `RNN` module is given: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n",
        "\n",
        "\n",
        "State Ã©s el quÃ¨ realment ens interessa--> Captures the RNN's summary of the entire sequence by encoding all the information processed up to the final time step. --> The state tensor is used when you only need the RNN's encoding of the entire sequence --> ens agrupa tota la frase que portem fi\n",
        "\n",
        "L'output en canvi -->  Contains the output of the RNN at each time step for every sequence in the batch --> tensor is used when you need the RNN's output at all time steps\n",
        "\n",
        "* The output tensor, containing the hidden states for all time steps, can be used for tasks where the intermediate hidden states are relevant, such as sequence labeling or machine translation.\n",
        "\n",
        "* The h_n tensor, containing the final hidden state, is useful for tasks where only the final representation of the sequence is needed, such as text classification or sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wAY1wWTT3pR"
      },
      "source": [
        "Now we can continue defining some more examples of RNN layers as promised before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ckyLF7PJVUrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2838f715-bf7f-4e57-a203-65b79d03ac9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN 2 layers >> ht.shape:  torch.Size([25, 5, 128])\n",
            "RNN 2 layers >> state.shape:  torch.Size([2, 5, 128])\n",
            "RNN 2 layers, batch_first >> ht.shape:  torch.Size([5, 25, 128])\n",
            "RNN 2 layers, batch_first >> state.shape:  torch.Size([2, 5, 128])\n"
          ]
        }
      ],
      "source": [
        "# 2 Layer RNN\n",
        "rnn2 = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=2)\n",
        "ht, state = rnn2(xt)\n",
        "print('RNN 2 layers >> ht.shape: ', ht.shape)\n",
        "print('RNN 2 layers >> state.shape: ', state.shape)\n",
        "\n",
        "# Batch Size first RNN\n",
        "xt_bf = torch.randn(BATCH_SIZE, SEQ_LEN, NUM_FEATS)\n",
        "rnn3 = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=2, batch_first=True)\n",
        "ht, state = rnn3(xt_bf)\n",
        "print('RNN 2 layers, batch_first >> ht.shape: ', ht.shape)\n",
        "print('RNN 2 layers, batch_first >> state.shape: ', state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNGrWTyorkNg"
      },
      "source": [
        "<p align=\"center\"><br>\n",
        "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/two_layers_rnn.png?raw=true\" class=\"center\" title=\"two layers RNN\" width=\"300\"/>\n",
        "</p><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ht shape depends on the batch_first setting. If batch_first=False, the shape is [sequence_length, batch_size, hidden_size].\n",
        "* If batch_first=True, the shape is [batch_size, sequence_length, hidden_size].\n",
        "state shape is always [num_layers, batch_size, hidden_size] regardless of the batch_first setting."
      ],
      "metadata": {
        "id": "bfd8FeD6edgF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfRkpNiOUwxA"
      },
      "source": [
        "### Exercise 3.1\n",
        "Build a **bidirectional RNN with 3 layers** by completing the TODO in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YdjkxY6yVUrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ab9d44-c5f5-42ba-b419-f352d62ee615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bidirectional RNN layer >> bi_ht.shape:  torch.Size([5, 25, 256])\n",
            "Bidirectional RNN layer >> bi_state.shape:  torch.Size([6, 5, 128])\n"
          ]
        }
      ],
      "source": [
        "# TODO: build the bidirectional RNN layer\n",
        "bi_rnn = rnn2 = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=3, bidirectional = True, batch_first=True)\n",
        "\n",
        "# forward xt_bf\n",
        "bi_ht, bi_state = bi_rnn(xt_bf)\n",
        "print('Bidirectional RNN layer >> bi_ht.shape: ', bi_ht.shape)\n",
        "print('Bidirectional RNN layer >> bi_state.shape: ', bi_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDE81Z2XKN2J"
      },
      "source": [
        "### Exercise 3.2\n",
        "What is the output $\\boldsymbol{h}_t$ shape and why?\n",
        "\n",
        "* bi_ht.shape:  torch.Size([5, 25, 256]): 5 mida batches, 25 mida seq, 256 perque Ã©s hidden state * 2\n",
        "\n",
        "### Exercise 3.3\n",
        "What is the output `state` shape and why?.\n",
        "\n",
        "\n",
        "* bi_state.shape: torch.Size([6, 5, 128]):\n",
        "nombre de capes = num_layers * 2 (bidirectional): 3 * 2 = 6\n",
        "mida batch: 5 i 128 nombre de capes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgKP_9eKTQcp"
      },
      "source": [
        "### Hold The Gates! A Recurrent Re-Evolution\n",
        "\n",
        "You've surely heard about the `LSTM` or the `GRU`, two practically sibling recurrent models. Well those are the actual RNNs you will use in your everyday. Why? Because they:\n",
        "1. Improve the memory capacity of the RNN.\n",
        "2. Improve the gradient flow of vanilla RNNs thanks to the learnable gate mechanisms.\n",
        "\n",
        "An LSTM or GRU cell is a composition of different neurons working jointly, and the whole thing replaces a single RNN neuron. The RNN cell (with one $\\tanh$ neuron), the LSTM cell and the GRU cell are depicted in the following figure from [this article](https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwiMiPbfoPHlAhUQCxoKHW9qA04Qjhx6BAgBEAI&url=http%3A%2F%2Fdprogrammer.org%2Frnn-lstm-gru&psig=AOvVaw3mU76KRvFfY9WiOF4N12ex&ust=1574080203478260):\n",
        "\n",
        "![lstm](http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU-1200x361.png)\n",
        "\n",
        "Now check that out. In the case of the LSTM, we have **two signals flowing in time** apart from the feed-forward input per time-step: $\\boldsymbol{c}_t$ and $\\boldsymbol{h}_t$. The first one is called the cumulative cell state. It basically will add everything it is \"allowed to see\" from the input, and will forget portions of it. This is unbounded. On the other hand, the output cell state $\\boldsymbol{h}_t$ will be the final layer activation (what is allowed to come out of it). This is bounded [-1, 1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WhMLsykdUEq"
      },
      "source": [
        "### Exercise 4: An LSTM Character-based Language Model\n",
        "\n",
        "In this final exercise we will train a language model that will work at the character level. This is, a neural network based on an RNN architecture that will complete language (textual) sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFGtDsWkUZoK"
      },
      "source": [
        "Our dataset will be composed of scripts from the *Friends* TV show. Download the episode 1 trainset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "B3XTLYUIVUrJ"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/telecombcn-dl/labs-all/master/labs/rnn/episode1_english.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "j387p07LVUrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae241f7-2b2b-43b3-ee20-95181550fa58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of found vocabulary tokens:  67\n"
          ]
        }
      ],
      "source": [
        "# Let's prepare some synthetic data\n",
        "\n",
        "def prepare_sequence(seq, char2idx, onehot=True):\n",
        "    # convert sequence of words to indices\n",
        "    idxs = [char2idx[c] for c in seq]\n",
        "    idxs = torch.tensor(idxs, dtype=torch.long)\n",
        "    if onehot:\n",
        "      # conver to onehot (if input to network)\n",
        "      ohs = F.one_hot(idxs, len(char2idx)).float()\n",
        "      return ohs\n",
        "    else:\n",
        "      return idxs\n",
        "\n",
        "with open('episode1_english.txt', 'r') as txt_f:\n",
        "  training_data = [l.rstrip() for l in txt_f if l.rstrip() != '']\n",
        "\n",
        "# merge the training data into one big text line\n",
        "training_data = '$'.join(training_data)\n",
        "\n",
        "# Assign a unique ID to each different character found in the training set\n",
        "char2idx = {}\n",
        "for c in training_data:\n",
        "    if c not in char2idx:\n",
        "        char2idx[c] = len(char2idx)\n",
        "idx2char = dict((v, k) for k, v in char2idx.items())\n",
        "VOCAB_SIZE = len(char2idx)\n",
        "RNN_SIZE = 1024\n",
        "MLP_SIZE = 2048\n",
        "SEQ_LEN = 50\n",
        "print('Number of found vocabulary tokens: ', VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehk_6JNIkZOs"
      },
      "source": [
        "##### Exercise 4.1\n",
        "* What is the amount of outputs needed by the character prediction model?\n",
        "* **size of vocabulary -> 67**\n",
        "\n",
        "##### Exercise 4.2\n",
        "* What is the proper activation to plug on top of the MLP (if any)? (Note that we use [`NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) later on).\n",
        "* **activation -> log soft max**\n",
        "\n",
        "##### Exercise 4.3\n",
        "* Finish the definition of the `CharLSTM` model to include a `nn.LSTM` layer, with `batch_first=True`, `vocab_size` inputs and `rnn_size` cells, and an MLP that projects the `rnn_size` to `mlp_size` with one `ReLU` hidden layer and then to the appropriate amount of outputs. Put a `Dropout(0.4)` after the `ReLU`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "omT3MsK4VUrK"
      },
      "outputs": [],
      "source": [
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, rnn_size, mlp_size):\n",
        "        super().__init__()\n",
        "        self.rnn_size = rnn_size\n",
        "\n",
        "        # TODO: Define the LSTM\n",
        "        self.lstm = nn.LSTM(vocab_size, rnn_size, batch_first=True)\n",
        "\n",
        "        self.dout = nn.Dropout(0.4)\n",
        "\n",
        "        # TODO: Create an MLP with a hidden layer of mlp_size neurons that maps\n",
        "        # from the RNN hidden state space to the output space of vocab_size\n",
        "        self.mlp = nn.Sequential(\n",
        "          # Linear layer\n",
        "          nn.Linear(rnn_size, mlp_size),\n",
        "          # Activation function\n",
        "          nn.ReLU(),\n",
        "          # Dropout (0.4)\n",
        "          self.dout,\n",
        "          # Linear layer\n",
        "          nn.Linear(mlp_size, vocab_size),\n",
        "          # Activation function\n",
        "          nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, sentence, state=None):\n",
        "        bsz, slen, vocab = sentence.shape\n",
        "        ht, state = self.lstm(sentence, state)\n",
        "        ht = self.dout(ht)\n",
        "        h = ht.contiguous().view(-1, self.rnn_size)\n",
        "        logprob = self.mlp(h)\n",
        "        return logprob, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K3L5BIyLrja"
      },
      "source": [
        "Test how the model performs when using randomly initialized weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8CWeShKbVUrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3e481b-0799-4df4-c96e-791cf19c4fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monica was AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n"
          ]
        }
      ],
      "source": [
        "# Let's build an example model and see what the scores are before training\n",
        "model = CharLSTM(VOCAB_SIZE, RNN_SIZE, MLP_SIZE)\n",
        "\n",
        "# This should output crap as it is not trained, so a fixed random tag for everything\n",
        "\n",
        "def gen_text(model, seed, char2idx, num_chars=150):\n",
        "  model.eval()\n",
        "  # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "  with torch.no_grad():\n",
        "      inputs = prepare_sequence(seed, char2idx)\n",
        "      # fill the RNN memory with the seed sentence\n",
        "      seed_pred, state = model(inputs.unsqueeze(0))\n",
        "      # now begin looping with feedback char by char from the last prediction\n",
        "      preds = seed\n",
        "      curr_pred = torch.argmax(seed_pred[-1, :])\n",
        "      curr_pred = idx2char[curr_pred.item()]\n",
        "      preds += curr_pred\n",
        "      for _ in range(num_chars):\n",
        "\n",
        "        # TODO: Get the next char prediction from the model given the current prediction and current state\n",
        "        inputs = prepare_sequence(curr_pred, char2idx)\n",
        "        curr_pred, state = model(inputs.unsqueeze(0), state)\n",
        "\n",
        "        curr_pred = torch.argmax(curr_pred[-1, :])\n",
        "        curr_pred = idx2char[curr_pred.item()]\n",
        "        if curr_pred == '$':\n",
        "          # special token to add newline char\n",
        "          preds += '\\n'\n",
        "        else:\n",
        "          preds += curr_pred\n",
        "      return preds\n",
        "\n",
        "\n",
        "print(gen_text(model, 'Monica was ', char2idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_HuhKqRL0p-"
      },
      "source": [
        "Prepare the training data by defining the data batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0kLEdO92VUrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69542144-95e1-4d90-99ab-91955b751f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training string len:  23149\n",
            "Sub-sequences len:  361\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "T = len(training_data)\n",
        "CHUNK_SIZE = T // BATCH_SIZE\n",
        "# let's first chunk the huge train sequence into BATCH_SIZE sub-sequences\n",
        "trainset = [training_data[beg_i:end_i] \\\n",
        "            for beg_i, end_i in zip(range(0, T - CHUNK_SIZE, CHUNK_SIZE),\n",
        "                                    range(CHUNK_SIZE, T, CHUNK_SIZE))]\n",
        "print('Original training string len: ', T)\n",
        "print('Sub-sequences len: ', CHUNK_SIZE)\n",
        "\n",
        "# The way training works is the following:\n",
        "# at each batch sampling from the trainset, we pick a portion of sequences\n",
        "# continuous with a sliding window in time. Hence, each of the BATCH_SIZE sub-sequences\n",
        "# in batch b[i] will continue in batch b[i + 1] in the same position of the batch dimension.\n",
        "# This is called stateful sampling, where we train with consecutive windows of sequences\n",
        "# We broke the long string into BATCH_SIZE subsequence, so we introduced BATCH_SIZE - 1\n",
        "# discontinuities... YES. But we can assume that each sub-sequence is continuous in a long\n",
        "# enough chunk so that those discontinuities are negligible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_VVeGkjk5bs"
      },
      "source": [
        "##### Exercise 4.4\n",
        "\n",
        "What is the length of the sliding window that will run over each of the training sub-sequences?\n",
        "\n",
        "NOTE: it is defined as a hyper-parameter above. How is this related to the backpropagation through time (BPTT)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-9RoEZyFVUrM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d9d1aaa-19b0-4242-d5c9-3f7d2bca543a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "They what it the sto the doon.  I just gonna be a hat here what it the with the with I don't the to the sour the store the start on the sto the doon.  I jus\n",
            "------------------------------\n",
            "Finished epoch 50 in 0.3 s: loss: 1.521642\n",
            "------------------------------\n",
            "They whilk you know anywaybe a see!\n",
            "[Time Lapse, Rachel is breating into a pant of Jour eof her sourd on the bast the coffee, grimace, and pour it into the \n",
            "------------------------------\n",
            "Finished epoch 100 in 0.3 s: loss: 0.200276\n",
            "------------------------------\n",
            "They ream.\n",
            "Monica: Well, it's like that. With feeling.\n",
            "Monica: (spilting furniture.]\n",
            "Ross: (sinting or Paul the Wine Guy?\n",
            "Ross: (lousting. You could get 'em\n",
            "------------------------------\n",
            "Finished epoch 150 in 0.3 s: loss: 0.038883\n",
            "------------------------------\n",
            "They here with Monica.\n",
            "Monica: Well, I'm just gonna get up and go to work.\n",
            "Rachel: Oh, look, wish me luck!\n",
            "Monica: What for?\n",
            "Rachel: I'm gonna go se poon. (\n",
            "------------------------------\n",
            "Finished epoch 200 in 0.3 s: loss: 0.024957\n",
            "------------------------------\n",
            "They hey well maye I con me in! you mean a beot me and you are, you are!\n",
            "Waitress: Can I get you some coffee.\n",
            "Ross: Thanks.\n",
            "Phoebe: Ooh, I just pulled out f\n",
            "------------------------------\n",
            "Finished epoch 250 in 0.3 s: loss: 0.020102\n",
            "------------------------------\n",
            "They here wo had starts welling tr the itchouce) That was a look dows. (thertalks that hister che perst to a hear bith.\n",
            "Ross: Monica's Apartment, Rachel is \n",
            "------------------------------\n",
            "Finished epoch 300 in 0.3 s: loss: 0.015725\n",
            "------------------------------\n",
            "They here with a lot out fort tee, and I saided have today ther.\n",
            "Andley: I lough what the Wend of wetch outtare work.\n",
            "[Tine: (he Subua here?\n",
            "Phoebe: Wait, d\n",
            "------------------------------\n",
            "Finished epoch 350 in 0.3 s: loss: 0.013060\n",
            "------------------------------\n",
            "They here wo leays. I have the alr a peal Dight we come going out to dinner and- not having sex.\n",
            "Chandler: So Rating the hormen she somang on the bookcase!\n",
            "\n",
            "------------------------------\n",
            "Finished epoch 400 in 0.3 s: loss: 0.011555\n",
            "------------------------------\n",
            "They hey- They was I like I was a job all right?\n",
            "Chandler: 'Look, Gippetto welching caush)\n",
            "Chandler: Al right, kids, I gotta get to work. If I don't input m\n",
            "------------------------------\n",
            "Finished epoch 450 in 0.3 s: loss: 0.012334\n",
            "------------------------------\n",
            "They hey, be that's the be wink of and do this goan a books of sile, this is . (They are all lined up ne pit have jobs Ross.\n",
            "Rachel: Hi, sure!\n",
            "Ross: Hi.\n",
            "(Th\n",
            "------------------------------\n",
            "Finished epoch 500 in 0.3 s: loss: 0.010831\n",
            "------------------------------\n",
            "They here to hel the pinteries, the ain werting a bean can s the little what ever happened to you! You got married, you mean with that.\n",
            "Monica: Chanden, And\n",
            "------------------------------\n",
            "Finished epoch 550 in 0.3 s: loss: 0.009309\n",
            "------------------------------\n",
            "They her- Puuld to just wan a livl a sedpyon, the phad creant for the watch.\n",
            "Monica: You and I gat mean I dot't need my parents, I've got grat boots' boots!\n",
            "------------------------------\n",
            "Finished epoch 600 in 0.3 s: loss: 0.007815\n",
            "------------------------------\n",
            "They really, I don't want to.\n",
            "Commercal Break\n",
            "[Scene: The Subway, Phoebe is singing from and Chandler are coming over to her me put together my new furnitur\n",
            "------------------------------\n",
            "Finished epoch 650 in 0.3 s: loss: 0.008917\n",
            "------------------------------\n",
            "They really, I'm than I firstered hame I's down't be is it.\n",
            "Chandler: (imitating the coffer) gring and realing the instructions) I'm suppos do a watch and r\n",
            "------------------------------\n",
            "Finished epoch 700 in 0.3 s: loss: 0.008153\n",
            "------------------------------\n",
            "They really, I'm puaring that has and part to your heffre...\n",
            "Paul: Thank you.\n",
            "Phoebe: You got screwed.\n",
            "Chandler: Oh my God!\n",
            "Paul: I know, I know, I'm such I\n",
            "------------------------------\n",
            "Finished epoch 750 in 0.3 s: loss: 0.006345\n",
            "------------------------------\n",
            "They really, so that hysterical phone can I get what s bord cut, cat, cut, cut, cut, cut..\n",
            "Chandler: (as Rachel is there.]\n",
            "Joey: (sitting on the arm of the \n",
            "------------------------------\n",
            "Finished epoch 800 in 0.3 s: loss: 0.007792\n",
            "------------------------------\n",
            "They right now, I was a lite bide, you a doad like that. With feeling a lot of part)\n",
            "Monica: So you wanna tell us now, or are we waitig to the others) Carol\n",
            "------------------------------\n",
            "Finished epoch 850 in 0.3 s: loss: 0.007472\n",
            "------------------------------\n",
            "They really not that has ex comel!\n",
            "Ross: Some the bouy, everybody, this is Rachel, another Lincol High survivor. (to Rachel) This is everybody, this is Rach\n",
            "------------------------------\n",
            "Finished epoch 900 in 0.3 s: loss: 0.007881\n",
            "------------------------------\n",
            "They here with Monica.\n",
            "Monica: Well, I'm singing wa've te ripped the ster whit if her and and talking.]\n",
            "Ross: (scornful) Grab a spoon!\n",
            "Ross: I honestly don'\n",
            "------------------------------\n",
            "Finished epoch 950 in 0.3 s: loss: 0.005788\n",
            "------------------------------\n",
            "They hey, I'm and I was looking at this gravy boat. This really not that has is probably not what you need right now what he's a dead man.  Oh, Chandler? (S\n",
            "------------------------------\n",
            "Finished epoch 1000 in 0.3 s: loss: 0.006736\n",
            "------------------------------\n",
            "They really necessary?  I mean, I can stop charging anytime I want.\n",
            "Monica: C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
            "------------------------------\n",
            "Finished epoch 1050 in 0.3 s: loss: 0.006317\n",
            "------------------------------\n",
            "They really, I don't want to.\n",
            "Commercial Break\n",
            "[Scene: Monica's Apartment, Rachel is talking and sines to the cangers and wank to the door and opens it to l\n",
            "------------------------------\n",
            "Finished epoch 1100 in 0.3 s: loss: 0.005572\n",
            "------------------------------\n",
            "They really, I don't want to I murk I was looking at this gravy boat. This really likes it when you rub her neck in all sore whing your enterthenter to how \n",
            "------------------------------\n",
            "Finished epoch 1150 in 0.3 s: loss: 0.005801\n",
            "------------------------------\n",
            "They really, I don't want to.\n",
            "Commercall Break\n",
            "[Scene: The Subway, Phoebe is singing from as Ross is leaving.)\n",
            "Monica: See ya.... Waitwait, wait, welling yo\n",
            "------------------------------\n",
            "Finished epoch 1200 in 0.3 s: loss: 0.004384\n",
            "------------------------------\n",
            "They hey- I'm an shor You're a shoe, you're a shoe, you're a shoe, you're a shoe, you're a shoe, you're a shoe, you're a shoe, you're a shoe, you're a shoe,\n",
            "------------------------------\n",
            "Finished epoch 1250 in 0.3 s: loss: 0.006148\n",
            "------------------------------\n",
            "They really, I don't want to be single, okay? Okay. I am just going to get up, go ah work.\n",
            "Monica: Oh wait, wait, unless you happened to catch the Reruns' p\n",
            "------------------------------\n",
            "Finished epoch 1300 in 0.3 s: loss: 0.006218\n",
            "------------------------------\n",
            "They here wo lo, do that I s donk a dong tay to this for and my lust that rean for the sithere and I didn't know anybody. And I ended up living over to hell\n",
            "------------------------------\n",
            "Finished epoch 1350 in 0.3 s: loss: 0.005981\n",
            "------------------------------\n",
            "They here was Monica's goeky olde brought breathe but Ross: I just want a millio dollars! (He extends his hand hopefully.)\n",
            "Monica: So how you doing today? D\n",
            "------------------------------\n",
            "Finished epoch 1400 in 0.3 s: loss: 0.002786\n",
            "------------------------------\n",
            "They here the coffe!   You can do anything! (Chandler slowly tries to candlers, and I realize I am totaly naked.\n",
            "All: Oh, yeah. Had that dream.\n",
            "Chandler: Oh\n",
            "------------------------------\n",
            "Finished epoch 1450 in 0.3 s: loss: 0.003922\n",
            "------------------------------\n",
            "They really, I don't want to be single, okay? Oh ... I always it! Um... my... father.\n",
            "[Scene: Monica and Paul are eating.]\n",
            "Monica: Whord God, I just want a \n",
            "------------------------------\n",
            "Finished epoch 1500 in 0.3 s: loss: 0.008620\n",
            "------------------------------\n",
            "They her where he'd how we wel out there is gonna beat in gots Lougher going to be incredibly lucky to become Mrs. Barry loks like Mr. Potato Head. Y'know, \n",
            "------------------------------\n",
            "Finished epoch 1550 in 0.3 s: loss: 0.003339\n",
            "------------------------------\n",
            "They here the coffee, grimace, and pour it into the universe.\n",
            "Monica: What's whit?\n",
            "Monica: That's Paul's watch and goes work.\n",
            "Monica: Oh wait, wait, unless \n",
            "------------------------------\n",
            "Finished epoch 1600 in 0.3 s: loss: 0.005359\n",
            "------------------------------\n",
            "They really not that wasn't a real date?! What the hell do mon! No, I know that some girl is there.]\n",
            "Monica: I hate you, I'm pushing my Aunt Roz through Par\n",
            "------------------------------\n",
            "Finished epoch 1650 in 0.3 s: loss: 0.003435\n",
            "------------------------------\n",
            "They reached down the stairs and everyone cheers.)\n",
            "Rachel: That' bey.\n",
            "Joey: You got screwed.\n",
            "Chandler: Oh my God!\n",
            "Paul: I know, I know, I'm such a hade that\n",
            "------------------------------\n",
            "Finished epoch 1700 in 0.3 s: loss: 0.003961\n",
            "------------------------------\n",
            "They here with Monica.\n",
            "Monica: Well, I'm singing that he bought her the big pipe organ, and sherediel outd to the wedding. I was me going this thand I just \n",
            "------------------------------\n",
            "Finished epoch 1750 in 0.3 s: loss: 0.004782\n",
            "------------------------------\n",
            "They hey, Monica!\n",
            "Monica: Hey Frannie, you have it, really, I don't want to.\n",
            "Commercall intos it in a plant.)\n",
            "Joey: Done with the bookcase!\n",
            "Chandler: Al rig\n",
            "------------------------------\n",
            "Finished epoch 1800 in 0.3 s: loss: 0.005165\n",
            "------------------------------\n",
            "They really not that has everybody thing that's it whing there's only one flavor of ice cream for you. Linge. Lookie sit the borkcoffe! So be her the best t\n",
            "------------------------------\n",
            "Finished epoch 1850 in 0.3 s: loss: 0.003759\n",
            "------------------------------\n",
            "They here with Monica.\n",
            "Monica: Well, I'm sinly 26 and I'm divorced!\n",
            "Joey: Shut up!\n",
            "Chandler: (Starts afters and my just for to Joey.\n",
            "Monica: I know, he's ju\n",
            "------------------------------\n",
            "Finished epoch 1900 in 0.3 s: loss: 0.003476\n",
            "------------------------------\n",
            "They really not that here and come to the room.)\n",
            "Chandler: And I just want a millio dollars! (He extends his hand hopefully.)\n",
            "Monica: So how you doing today\n",
            "------------------------------\n",
            "Finished epoch 1950 in 0.3 s: loss: 0.004108\n",
            "------------------------------\n",
            "They hand I said, 'What if you geed you same of these wet right what I Was a lesbian... (They al stare at him.) Did I say that out loud?\n",
            "Ross: I thinks...\n",
            "P\n",
            "------------------------------\n",
            "Finished epoch 2000 in 0.3 s: loss: 0.003359\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'NLLLoss')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's now build a model to train with its optimizer and loss\n",
        "model = CharLSTM(VOCAB_SIZE, RNN_SIZE, MLP_SIZE)\n",
        "model.to(device)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "NUM_EPOCHS = 2000\n",
        "tr_loss = []\n",
        "state = None\n",
        "timer_beg = timer()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  model.train()\n",
        "  # let's slide over our dataset\n",
        "  for beg_t, end_t in zip(range(0, CHUNK_SIZE - SEQ_LEN - 1, SEQ_LEN + 1),\n",
        "                          range(SEQ_LEN + 1, CHUNK_SIZE, SEQ_LEN + 1)):\n",
        "    # Step 1. Remember that Pytorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "    # Tensors of one-hot sequences.\n",
        "    for sent in trainset:\n",
        "      # chunk the sentence\n",
        "      chunk = sent[beg_t:end_t]\n",
        "\n",
        "      # TODO: get X and Y with a shift of 1\n",
        "      X = chunk[:-1]    # remove last char\n",
        "      Y = chunk[1:]     # remove first char\n",
        "\n",
        "      # convert each sequence to one-hots and labels respectively\n",
        "      X = prepare_sequence(X, char2idx)\n",
        "      Y = prepare_sequence(Y, char2idx, onehot=False)\n",
        "      dataX.append(X.unsqueeze(0)) # create batch-dim\n",
        "      dataY.append(Y.unsqueeze(0)) # create batch-dim\n",
        "    dataX = torch.cat(dataX, dim=0).to(device)\n",
        "    dataY = torch.cat(dataY, dim=0).to(device)\n",
        "\n",
        "    # Step 3. Run our forward pass.\n",
        "    # Forward through model and carry the previous state forward in time (statefulness)\n",
        "    y_, state = model(dataX, state)\n",
        "    # detach the previous state graph to not backprop gradients further than the BPTT span\n",
        "    state = (state[0].detach(), # detach c[t]\n",
        "             state[1].detach()) # detach h[t]\n",
        "\n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "    #  calling optimizer.step()\n",
        "    loss = loss_function(y_, dataY.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tr_loss.append(loss.item())\n",
        "  timer_end = timer()\n",
        "  if (epoch + 1) % 50 == 0:\n",
        "    # Generate a seed sentence to play around\n",
        "    model.to('cpu')\n",
        "    print('-' * 30)\n",
        "    print(gen_text(model, 'They ', char2idx))\n",
        "    print('-' * 30)\n",
        "    model.to(device)\n",
        "    print('Finished epoch {} in {:.1f} s: loss: {:.6f}'.format(epoch + 1,\n",
        "                                                               timer_end - timer_beg,\n",
        "                                                               np.mean(tr_loss[-10:])))\n",
        "  timer_beg = timer()\n",
        "\n",
        "plt.plot(tr_loss)\n",
        "plt.xlabel('Update')\n",
        "plt.ylabel('NLLLoss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iulcm9gPNhwK"
      },
      "source": [
        "Now that the generator of characters is trained, we can ask it to predict the rest of a sentence that begins with `Professor `:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ooYoQ2U2VUrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0bae27-87f0-4fbe-b996-73b20277c6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Professor Paul's watch and goes the of the coffe!  I'm some that is and 'L'ss a sembong.\n",
            "Ross: Thongs, I'm s reddidg you can hear a thousand retailers scream.\n",
            "(She sith stare working at this gravy boat. This really likes it when you rub her neck in his world finilly, do that hesterical phone.]\n",
            "Ross: No, no, I'm not ready!  Hight Rachel is there.]\n",
            "Joey: (sitting on the arm of the coffe!)\n",
            "Monica: So how you doing today? Did you are!\n",
            "Waitress: Can I get you some coffee.\n",
            "Ross: Thanks.\n",
            "Phoebe: Ooh, I just pulled out four eyelahe . Thow can't be good.\n",
            "(Monica goes to change.)\n",
            "Chandler: Angela's And the park.\n",
            "Joey: Look, that's it 6:3?\n",
            "Ross: I'll be fine, alright? Real... there aske thing.]\n",
            "Frannie: You're with Paul.\n",
            "Rachel: Hello, Paul.\n",
            "Chandler: Are you kidding? I'm trained or loth of siles here stof a mear brot. (She starts massaging them.)\n",
            "Monica: I just thought I was Monica's geeky older and they all cheer.)\n",
            "Monica: Shertaing, I was a wesber you are!\n",
            "Waitress: Can I get you some coffee.\n",
            "Ross: Thank\n"
          ]
        }
      ],
      "source": [
        "print(gen_text(model.to('cpu'), 'Professor ', char2idx, 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVRsFgHZa3f3"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3.7.10 ('trading')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "148c72a8fa8931f1b4adec61e5c626da15d84fdba20a1a50eaf0317d3b0337d5"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}